"""
ä»»åŠ¡è°ƒåº¦æ¨¡å—
å®ç°å®šæ—¶æ‰§è¡Œçˆ¬å–å’Œæ¨é€ä»»åŠ¡
"""
import asyncio
from datetime import datetime, timezone
from typing import List, Callable, Optional
from apscheduler.schedulers.asyncio import AsyncIOScheduler
from apscheduler.triggers.cron import CronTrigger
from apscheduler.events import EVENT_JOB_EXECUTED, EVENT_JOB_ERROR
from loguru import logger
import pytz

from ..config import get_config
from ..crawlers.base import create_crawler
from ..processors.content_processor import ContentProcessor
from ..generators.web_generator import WebGenerator
from ..services.statistics import StatisticsService
from ..models import NewsItem

class TaskScheduler:
    """ä»»åŠ¡è°ƒåº¦å™¨"""
    
    def __init__(self):
        self.config = get_config()
        self.scheduler = AsyncIOScheduler(timezone=pytz.timezone(self.config.scheduler.timezone))
        self.processor = ContentProcessor()
        self.web_generator = WebGenerator()
        self.stats_service = StatisticsService()
        self.is_running = False
        
        # è®¾ç½®äº‹ä»¶ç›‘å¬å™¨
        self.scheduler.add_listener(self._job_executed, EVENT_JOB_EXECUTED)
        self.scheduler.add_listener(self._job_error, EVENT_JOB_ERROR)
    
    def _job_executed(self, event):
        """ä»»åŠ¡æ‰§è¡ŒæˆåŠŸå›è°ƒ"""
        logger.info(f"ä»»åŠ¡æ‰§è¡ŒæˆåŠŸ: {event.job_id}")
    
    def _job_error(self, event):
        """ä»»åŠ¡æ‰§è¡Œå¤±è´¥å›è°ƒ"""
        logger.error(f"ä»»åŠ¡æ‰§è¡Œå¤±è´¥: {event.job_id}, å¼‚å¸¸: {event.exception}")
    
    async def crawl_all_sources(self) -> List[NewsItem]:
        """çˆ¬å–æ‰€æœ‰èµ„è®¯æº"""
        logger.info("å¼€å§‹æ‰§è¡Œæ¯æ—¥èµ„è®¯çˆ¬å–ä»»åŠ¡")
        
        all_items = []
        enabled_sources = [source for source in self.config.sources if source.enabled]
        
        logger.info(f"å¯ç”¨çš„èµ„è®¯æºæ•°é‡: {len(enabled_sources)}")
        
        # å¹¶å‘çˆ¬å–æ‰€æœ‰èµ„è®¯æº
        crawl_tasks = []
        for source in enabled_sources:
            crawler = create_crawler(source)
            crawl_tasks.append(self._crawl_single_source(crawler))
        
        # ç­‰å¾…æ‰€æœ‰çˆ¬å–ä»»åŠ¡å®Œæˆ
        results = await asyncio.gather(*crawl_tasks, return_exceptions=True)
        
        # æ”¶é›†ç»“æœ
        for i, result in enumerate(results):
            if isinstance(result, Exception):
                logger.error(f"çˆ¬å–æº {enabled_sources[i].name} å¤±è´¥: {result}")
            elif isinstance(result, list):
                all_items.extend(result)
                logger.info(f"çˆ¬å–æº {enabled_sources[i].name} æˆåŠŸ: {len(result)} æ¡")
        
        logger.success(f"çˆ¬å–ä»»åŠ¡å®Œæˆï¼Œå…±è·å¾— {len(all_items)} æ¡èµ„è®¯")
        return all_items
    
    async def _crawl_single_source(self, crawler) -> List[NewsItem]:
        """çˆ¬å–å•ä¸ªèµ„è®¯æº"""
        try:
            # åœ¨æ–°çš„çº¿ç¨‹ä¸­æ‰§è¡ŒåŒæ­¥çˆ¬å–æ“ä½œ
            loop = asyncio.get_event_loop()
            items = await loop.run_in_executor(None, crawler.crawl)
            return items
        except Exception as e:
            logger.error(f"çˆ¬å–å¤±è´¥: {e}")
            return []
    
    async def daily_digest_task(self):
        """æ¯æ—¥æ‘˜è¦ä»»åŠ¡"""
        try:
            logger.info("å¼€å§‹æ‰§è¡Œæ¯æ—¥æ‘˜è¦ä»»åŠ¡")
            
            # æ­¥éª¤1: çˆ¬å–æ‰€æœ‰èµ„è®¯
            logger.info("æ­¥éª¤1: çˆ¬å–æ‰€æœ‰èµ„è®¯...")
            items = await self.crawl_all_sources()
            
            if not items:
                logger.warning("æ²¡æœ‰è·å–åˆ°ä»»ä½•èµ„è®¯ï¼Œè·³è¿‡æŠ¥å‘Šç”Ÿæˆ")
                return
            
            logger.info(f"æˆåŠŸçˆ¬å– {len(items)} æ¡èµ„è®¯")
            
            # æ­¥éª¤2: å¤„ç†å†…å®¹
            logger.info("æ­¥éª¤2: å¤„ç†å†…å®¹...")
            processed_items = self.processor.process_items(items)
            
            if not processed_items:
                logger.warning("å†…å®¹å¤„ç†åæ²¡æœ‰æœ‰æ•ˆèµ„è®¯ï¼Œè·³è¿‡æŠ¥å‘Šç”Ÿæˆ")
                return
                
            logger.info(f"æˆåŠŸå¤„ç† {len(processed_items)} æ¡æœ‰æ•ˆèµ„è®¯")
            
            # æ­¥éª¤3: ç”Ÿæˆæ¯æ—¥æ‘˜è¦...
            logger.info("æ­¥éª¤3: ç”Ÿæˆæ¯æ—¥æ‘˜è¦...")
            daily_digest = self.processor.create_daily_digest(processed_items)
            
            # æ­¥éª¤4: ç”Ÿæˆç½‘é¡µ
            logger.info("æ­¥éª¤4: ç”Ÿæˆç½‘é¡µ...")
            result = self.web_generator.generate_complete_site(daily_digest)
            
            if not result:
                logger.error("ç½‘é¡µç”Ÿæˆå¤±è´¥")
                return
                
            logger.success(f"ç½‘é¡µç”ŸæˆæˆåŠŸ: {result['output_dir']}")
            logger.info(f"é¦–é¡µ: {result['index_page']}")
            logger.info(f"æ¯æ—¥é¡µé¢: {result['daily_page']}")
            logger.info(f"å½’æ¡£é¡µé¢: {result['archive_page']}")
            
            # æ­¥éª¤5: ç”Ÿæˆç»Ÿè®¡æŠ¥å‘Š
            logger.info("æ­¥éª¤5: ç”Ÿæˆç»Ÿè®¡æŠ¥å‘Š...")
            try:
                self.stats_service.save_statistics()
                
                # ç®€å•ç»Ÿè®¡
                basic_stats = self.stats_service.get_basic_statistics()
                logger.info(
                    f"ğŸ“Š ç»Ÿè®¡æŠ¥å‘Š: å…±å¤„ç† {basic_stats.get('total_articles', 0)} æ¡èµ„è®¯"
                )
                          
            except Exception as e:
                logger.warning(f"ç»Ÿè®¡æŠ¥å‘Šç”Ÿæˆå¤±è´¥: {e}")
            
            # å¯ä»¥åœ¨è¿™é‡Œæ·»åŠ éƒ¨ç½²é€»è¾‘
            # await self._deploy_to_github_pages(result['output_dir'])
                
        except KeyboardInterrupt:
            logger.info("ç”¨æˆ·ä¸­æ–­ä»»åŠ¡æ‰§è¡Œ")
            raise
        except Exception as e:
            logger.error(f"æ¯æ—¥æ‘˜è¦ä»»åŠ¡æ‰§è¡Œå¤±è´¥: {e}")
            import traceback
            logger.debug(f"è¯¦ç»†é”™è¯¯ä¿¡æ¯: {traceback.format_exc()}")
            raise
    
    async def _send_completion_notification(self, digest, site_url: str):
        """å‘é€å®Œæˆé€šçŸ¥"""
        try:
            notification_content = f"""
ğŸ“… {digest.date.strftime('%Yå¹´%mæœˆ%dæ—¥')} è®¾è®¡èµ„è®¯æ—¥æŠ¥å·²ç”Ÿæˆ
ğŸ“Š å…±æ”¶é›† {digest.total_items} æ¡èµ„è®¯
ğŸŒ æ¥æºç½‘ç«™ {len(digest.sources)} ä¸ª
ğŸ”— æŸ¥çœ‹ç½‘ç«™: {site_url}
            """.strip()
            
            # è¿™é‡Œå¯ä»¥æ·»åŠ å…¶ä»–é€šçŸ¥æ–¹å¼ï¼Œæ¯”å¦‚é‚®ä»¶ã€Webhookç­‰
            logger.info("å®Œæˆé€šçŸ¥å·²å‘é€")
            
        except Exception as e:
            logger.error(f"å‘é€å®Œæˆé€šçŸ¥å¤±è´¥: {e}")
    
    def add_daily_job(self):
        """æ·»åŠ æ¯æ—¥ä»»åŠ¡"""
        trigger = CronTrigger(
            hour=self.config.scheduler.hour,
            minute=self.config.scheduler.minute,
            timezone=self.config.scheduler.timezone
        )
        
        self.scheduler.add_job(
            self.daily_digest_task,
            trigger=trigger,
            id="daily_digest",
            name="æ¯æ—¥è®¾è®¡èµ„è®¯æ‘˜è¦",
            replace_existing=True
        )
        
        logger.info(f"æ¯æ—¥ä»»åŠ¡å·²æ·»åŠ : {self.config.scheduler.hour}:{self.config.scheduler.minute:02d}")
    
    def add_custom_job(self, func: Callable, trigger, job_id: str, name: str = None):
        """æ·»åŠ è‡ªå®šä¹‰ä»»åŠ¡"""
        self.scheduler.add_job(
            func,
            trigger=trigger,
            id=job_id,
            name=name or job_id,
            replace_existing=True
        )
        logger.info(f"è‡ªå®šä¹‰ä»»åŠ¡å·²æ·»åŠ : {job_id}")
    
    def remove_job(self, job_id: str):
        """ç§»é™¤ä»»åŠ¡"""
        try:
            self.scheduler.remove_job(job_id)
            logger.info(f"ä»»åŠ¡å·²ç§»é™¤: {job_id}")
        except Exception as e:
            logger.error(f"ç§»é™¤ä»»åŠ¡å¤±è´¥: {e}")
    
    def list_jobs(self):
        """åˆ—å‡ºæ‰€æœ‰ä»»åŠ¡"""
        jobs = self.scheduler.get_jobs()
        logger.info(f"å½“å‰ä»»åŠ¡æ•°é‡: {len(jobs)}")
        
        for job in jobs:
            next_run = job.next_run_time.strftime('%Y-%m-%d %H:%M:%S') if job.next_run_time else "N/A"
            logger.info(f"ä»»åŠ¡: {job.id} | åç§°: {job.name} | ä¸‹æ¬¡è¿è¡Œ: {next_run}")
    
    def start(self):
        """å¯åŠ¨è°ƒåº¦å™¨"""
        if not self.is_running:
            self.scheduler.start()
            self.is_running = True
            logger.success("ä»»åŠ¡è°ƒåº¦å™¨å·²å¯åŠ¨")
        else:
            logger.warning("ä»»åŠ¡è°ƒåº¦å™¨å·²åœ¨è¿è¡Œä¸­")
    
    def stop(self):
        """åœæ­¢è°ƒåº¦å™¨"""
        if self.is_running:
            self.scheduler.shutdown()
            self.is_running = False
            logger.info("ä»»åŠ¡è°ƒåº¦å™¨å·²åœæ­¢")
        else:
            logger.warning("ä»»åŠ¡è°ƒåº¦å™¨æœªåœ¨è¿è¡Œ")
    
    async def run_once(self):
        """ç«‹å³æ‰§è¡Œä¸€æ¬¡ä»»åŠ¡ï¼ˆç”¨äºæµ‹è¯•ï¼‰"""
        logger.info("ç«‹å³æ‰§è¡Œä¸€æ¬¡æ¯æ—¥æ‘˜è¦ä»»åŠ¡")
        await self.daily_digest_task()
    
    async def test_all_sources(self):
        """æµ‹è¯•æ‰€æœ‰èµ„è®¯æº"""
        logger.info("å¼€å§‹æµ‹è¯•æ‰€æœ‰èµ„è®¯æº")
        
        enabled_sources = [source for source in self.config.sources if source.enabled]
        
        for source in enabled_sources:
            try:
                logger.info(f"æµ‹è¯•èµ„è®¯æº: {source.name}")
                crawler = create_crawler(source)
                items = await self._crawl_single_source(crawler)
                
                if items:
                    logger.success(f"âœ… {source.name}: æˆåŠŸè·å– {len(items)} æ¡èµ„è®¯")
                    # æ˜¾ç¤ºç¬¬ä¸€æ¡ä½œä¸ºç¤ºä¾‹
                    if len(items) > 0:
                        first_item = items[0]
                        logger.info(f"   ç¤ºä¾‹: {first_item.title}")
                else:
                    logger.warning(f"âš ï¸ {source.name}: æœªè·å–åˆ°èµ„è®¯")
                    
            except Exception as e:
                logger.error(f"âŒ {source.name}: æµ‹è¯•å¤±è´¥ - {e}")
        
        logger.info("èµ„è®¯æºæµ‹è¯•å®Œæˆ")
    
    async def test_web_generation(self):
        """æµ‹è¯•ç½‘é¡µç”ŸæˆåŠŸèƒ½"""
        logger.info("æµ‹è¯•ç½‘é¡µç”ŸæˆåŠŸèƒ½")
        
        try:
                        # åˆ›å»ºæµ‹è¯•æ•°æ®
            from datetime import datetime
            from ..models import NewsItem, DailyDigest, CategorySummary
            
            test_items = [
                NewsItem(
                    title="æµ‹è¯•è®¾è®¡èµ„è®¯1",
                    url="https://example.com/1",
                    author=None,
                    category="è®¾è®¡èµ„è®¯",
                    source="æµ‹è¯•æº",
                    image_url=None,
                    summary="è¿™æ˜¯ä¸€æ¡æµ‹è¯•èµ„è®¯çš„æ‘˜è¦å†…å®¹",
                    tags=[],
                    stats=None,
                    published_at=None
                ),
                NewsItem(
                    title="æµ‹è¯•è®¾è®¡èµ„è®¯2", 
                    url="https://example.com/2",
                    author=None,
                    category="ä½œå“å±•ç¤º",
                    source="æµ‹è¯•æº",
                    image_url=None,
                    summary="è¿™æ˜¯å¦ä¸€æ¡æµ‹è¯•èµ„è®¯çš„æ‘˜è¦å†…å®¹",
                    tags=[],
                    stats=None,
                    published_at=None
                )
            ]
            
            test_digest = self.processor.create_daily_digest(test_items)
            
            # æµ‹è¯•ç”Ÿæˆç½‘é¡µ
            result = self.web_generator.generate_complete_site(test_digest)
            
            if result:
                logger.success("âœ… ç½‘é¡µç”ŸæˆåŠŸèƒ½æ­£å¸¸")
                logger.info(f"è¾“å‡ºç›®å½•: {result['output_dir']}")
                return True
            else:
                logger.error("âŒ ç½‘é¡µç”Ÿæˆå¤±è´¥")
                return False
                
        except Exception as e:
            logger.error(f"âŒ ç½‘é¡µç”Ÿæˆæµ‹è¯•å¼‚å¸¸: {e}")
            return False 