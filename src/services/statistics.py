#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
ç»Ÿè®¡æœåŠ¡æ¨¡å—
æä¾›æ•°æ®ç»Ÿè®¡å’Œåˆ†æç»“æœå±•ç¤ºåŠŸèƒ½
"""

import json
import os
from datetime import datetime, timedelta
from collections import Counter, defaultdict
from typing import Dict, List, Any
import logging

logger = logging.getLogger(__name__)


class StatisticsService:
    """ç»Ÿè®¡æœåŠ¡ç±»"""
    
    def __init__(self):
        self.data_dir = "data"
        self.feeds_file = os.path.join(self.data_dir, "feeds_data.json")
        self.ai_analysis_file = os.path.join(self.data_dir, "ai_analysis.json")
        
    def load_feeds_data(self) -> Dict:
        """åŠ è½½RSSæ•°æ®"""
        try:
            if os.path.exists(self.feeds_file):
                with open(self.feeds_file, 'r', encoding='utf-8') as f:
                    return json.load(f)
        except Exception as e:
            logger.error(f"åŠ è½½RSSæ•°æ®å¤±è´¥: {e}")
        return {}
    
    def load_ai_analysis(self) -> Dict:
        """åŠ è½½AIåˆ†ææ•°æ®"""
        try:
            if os.path.exists(self.ai_analysis_file):
                with open(self.ai_analysis_file, 'r', encoding='utf-8') as f:
                    return json.load(f)
        except Exception as e:
            logger.error(f"åŠ è½½AIåˆ†ææ•°æ®å¤±è´¥: {e}")
        return {}
    
    def get_basic_statistics(self) -> Dict[str, Any]:
        """è·å–åŸºç¡€ç»Ÿè®¡ä¿¡æ¯"""
        feeds_data = self.load_feeds_data()
        ai_analysis = self.load_ai_analysis()
        
        stats = {
            'total_sources': 0,
            'total_articles': 0,
            'articles_with_ai': 0,
            'ai_success_rate': 0,
            'sources_stats': {},
            'update_time': None
        }
        
        if not feeds_data:
            return stats
        
        # åŸºç¡€ç»Ÿè®¡
        stats['total_sources'] = len(feeds_data.get('sources', {}))
        
        articles_count = 0
        ai_count = 0
        sources_stats = {}
        
        for source_name, source_data in feeds_data.get('sources', {}).items():
            items = source_data.get('items', [])
            articles_count += len(items)
            
            # ç»Ÿè®¡è¯¥æºçš„AIåˆ†ææ•°é‡
            ai_items = sum(1 for item in items if item.get('link') in ai_analysis)
            ai_count += ai_items
            
            sources_stats[source_name] = {
                'total_articles': len(items),
                'ai_analyzed': ai_items,
                'success_rate': (ai_items / len(items) * 100) if items else 0
            }
        
        stats['total_articles'] = articles_count
        stats['articles_with_ai'] = ai_count
        stats['ai_success_rate'] = (ai_count / articles_count * 100) if articles_count > 0 else 0
        stats['sources_stats'] = sources_stats
        stats['update_time'] = feeds_data.get('generated_at')
        
        return stats
    
    def get_content_analysis(self) -> Dict[str, Any]:
        """è·å–å†…å®¹åˆ†æç»Ÿè®¡"""
        ai_analysis = self.load_ai_analysis()
        
        if not ai_analysis:
            return {}
        
        # åˆ†ææ‘˜è¦é•¿åº¦åˆ†å¸ƒ
        summary_lengths = [len(data.get('summary', '')) for data in ai_analysis.values()]
        
        # åˆ†æå…³é”®è¯é¢‘ç‡ï¼ˆç®€å•çš„è¯é¢‘ç»Ÿè®¡ï¼‰
        all_summaries = ' '.join([data.get('summary', '') for data in ai_analysis.values()])
        
        analysis = {
            'total_summaries': len(ai_analysis),
            'avg_summary_length': sum(summary_lengths) / len(summary_lengths) if summary_lengths else 0,
            'min_summary_length': min(summary_lengths) if summary_lengths else 0,
            'max_summary_length': max(summary_lengths) if summary_lengths else 0,
            'summary_length_distribution': self._get_length_distribution(summary_lengths)
        }
        
        return analysis
    
    def _get_length_distribution(self, lengths: List[int]) -> Dict[str, int]:
        """è·å–é•¿åº¦åˆ†å¸ƒ"""
        distribution = {
            'very_short': 0,  # < 50å­—ç¬¦
            'short': 0,       # 50-100å­—ç¬¦
            'medium': 0,      # 100-200å­—ç¬¦
            'long': 0,        # 200-300å­—ç¬¦
            'very_long': 0    # > 300å­—ç¬¦
        }
        
        for length in lengths:
            if length < 50:
                distribution['very_short'] += 1
            elif length < 100:
                distribution['short'] += 1
            elif length < 200:
                distribution['medium'] += 1
            elif length < 300:
                distribution['long'] += 1
            else:
                distribution['very_long'] += 1
        
        return distribution
    
    def get_time_analysis(self) -> Dict[str, Any]:
        """è·å–æ—¶é—´åˆ†æç»Ÿè®¡"""
        feeds_data = self.load_feeds_data()
        
        if not feeds_data:
            return {}
        
        # æ”¶é›†æ‰€æœ‰æ–‡ç« çš„å‘å¸ƒæ—¶é—´
        articles_by_hour = defaultdict(int)
        articles_by_day = defaultdict(int)
        recent_articles = 0
        
        now = datetime.now()
        one_day_ago = now - timedelta(days=1)
        
        for source_data in feeds_data.get('sources', {}).values():
            for item in source_data.get('items', []):
                pub_date = item.get('published_parsed')
                if pub_date:
                    try:
                        # è½¬æ¢ä¸ºdatetimeå¯¹è±¡
                        dt = datetime(*pub_date[:6])
                        hour = dt.hour
                        day = dt.strftime('%Y-%m-%d')
                        
                        articles_by_hour[hour] += 1
                        articles_by_day[day] += 1
                        
                        # ç»Ÿè®¡æœ€è¿‘24å°æ—¶çš„æ–‡ç« 
                        if dt > one_day_ago:
                            recent_articles += 1
                            
                    except Exception:
                        continue
        
        # æ‰¾å‡ºæœ€æ´»è·ƒçš„æ—¶é—´æ®µ
        peak_hour = max(articles_by_hour.items(), key=lambda x: x[1]) if articles_by_hour else (0, 0)
        
        return {
            'articles_by_hour': dict(articles_by_hour),
            'articles_by_day': dict(articles_by_day),
            'peak_hour': peak_hour[0],
            'peak_hour_count': peak_hour[1],
            'recent_articles_24h': recent_articles,
            'total_days_covered': len(articles_by_day)
        }
    
    def generate_report(self) -> str:
        """ç”Ÿæˆç»Ÿè®¡æŠ¥å‘Š"""
        basic_stats = self.get_basic_statistics()
        content_stats = self.get_content_analysis()
        time_stats = self.get_time_analysis()
        
        report = []
        report.append("ğŸ“Š æ¨é€æœºå™¨äººç»Ÿè®¡æŠ¥å‘Š")
        report.append("=" * 50)
        
        # åŸºç¡€ç»Ÿè®¡
        report.append(f"\nğŸ”¢ åŸºç¡€ç»Ÿè®¡:")
        report.append(f"  â€¢ RSSæºæ•°é‡: {basic_stats['total_sources']}")
        report.append(f"  â€¢ æ–‡ç« æ€»æ•°: {basic_stats['total_articles']}")
        report.append(f"  â€¢ AIåˆ†ææ•°é‡: {basic_stats['articles_with_ai']}")
        report.append(f"  â€¢ AIæˆåŠŸç‡: {basic_stats['ai_success_rate']:.1f}%")
        
        if basic_stats['update_time']:
            report.append(f"  â€¢ æœ€åæ›´æ–°: {basic_stats['update_time']}")
        
        # å„æºç»Ÿè®¡
        if basic_stats['sources_stats']:
            report.append(f"\nğŸ“ˆ å„RSSæºç»Ÿè®¡:")
            for source, stats in basic_stats['sources_stats'].items():
                report.append(f"  â€¢ {source}: {stats['total_articles']}ç¯‡æ–‡ç« , "
                            f"AIåˆ†æ {stats['ai_analyzed']}ç¯‡ ({stats['success_rate']:.1f}%)")
        
        # å†…å®¹åˆ†æ
        if content_stats:
            report.append(f"\nğŸ“ å†…å®¹åˆ†æ:")
            report.append(f"  â€¢ å¹³å‡æ‘˜è¦é•¿åº¦: {content_stats['avg_summary_length']:.1f}å­—ç¬¦")
            report.append(f"  â€¢ æ‘˜è¦é•¿åº¦èŒƒå›´: {content_stats['min_summary_length']}-{content_stats['max_summary_length']}å­—ç¬¦")
            
            dist = content_stats['summary_length_distribution']
            report.append(f"  â€¢ é•¿åº¦åˆ†å¸ƒ: å¾ˆçŸ­({dist['very_short']}) | "
                        f"çŸ­({dist['short']}) | ä¸­ç­‰({dist['medium']}) | "
                        f"é•¿({dist['long']}) | å¾ˆé•¿({dist['very_long']})")
        
        # æ—¶é—´åˆ†æ
        if time_stats:
            report.append(f"\nâ° æ—¶é—´åˆ†æ:")
            report.append(f"  â€¢ è¦†ç›–å¤©æ•°: {time_stats['total_days_covered']}å¤©")
            report.append(f"  â€¢ æœ€è¿‘24å°æ—¶æ–‡ç« : {time_stats['recent_articles_24h']}ç¯‡")
            report.append(f"  â€¢ é«˜å³°æ—¶æ®µ: {time_stats['peak_hour']}ç‚¹ ({time_stats['peak_hour_count']}ç¯‡æ–‡ç« )")
        
        report.append("\n" + "=" * 50)
        
        return "\n".join(report)
    
    def save_statistics(self, output_path: str = None):
        """ä¿å­˜ç»Ÿè®¡ä¿¡æ¯åˆ°æ–‡ä»¶"""
        if not output_path:
            output_path = os.path.join("output", "statistics.txt")
        
        os.makedirs(os.path.dirname(output_path), exist_ok=True)
        
        report = self.generate_report()
        
        with open(output_path, 'w', encoding='utf-8') as f:
            f.write(report)
        
        logger.info(f"ç»Ÿè®¡æŠ¥å‘Šå·²ä¿å­˜åˆ°: {output_path}")
        
        return output_path
    
    def print_statistics(self):
        """æ‰“å°ç»Ÿè®¡ä¿¡æ¯åˆ°æ§åˆ¶å°"""
        print(self.generate_report())


def main():
    """æµ‹è¯•å‡½æ•°"""
    stats_service = StatisticsService()
    stats_service.print_statistics()


if __name__ == "__main__":
    main() 