#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
è¿è¡Œçˆ¬è™«è„šæœ¬
æ‰‹åŠ¨è¿è¡Œçˆ¬è™«ç³»ç»Ÿè·å–å½“å¤©çœŸå®èµ„è®¯
"""

import sys
import asyncio
from pathlib import Path
from datetime import datetime

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root / "src"))

from src.config import get_config
from src.scheduler.task_scheduler import TaskScheduler
from src.generators.web_generator import WebGenerator

async def run_crawler():
    """è¿è¡Œçˆ¬è™«ç³»ç»Ÿ"""
    try:
        print("ğŸš€ å¼€å§‹è¿è¡Œçˆ¬è™«ç³»ç»Ÿ...")
        print("=" * 60)
        
        # åˆ›å»ºä»»åŠ¡è°ƒåº¦å™¨
        scheduler = TaskScheduler()
        
        # æ‰§è¡Œæ¯æ—¥æ‘˜è¦ä»»åŠ¡ï¼ˆè¿™ä¼šçˆ¬å–çœŸå®èµ„è®¯ï¼‰
        print("ğŸ“… å¼€å§‹çˆ¬å–ä»Šæ—¥èµ„è®¯...")
        await scheduler.daily_digest_task()
        
        print("\nğŸ‰ çˆ¬è™«è¿è¡Œå®Œæˆï¼")
        print("ğŸ“„ ç”Ÿæˆçš„æ–‡ä»¶:")
        print("   â€¢ data/digest-*.json - æ•°æ®æ–‡ä»¶")
        print("   â€¢ output/daily-*.html - æ¯æ—¥é¡µé¢")
        print("   â€¢ output/index.html - é¦–é¡µ")
        print("   â€¢ output/archive.html - å½’æ¡£é¡µé¢")
        
        return True
        
    except Exception as e:
        print(f"\nâŒ çˆ¬è™«è¿è¡Œå¤±è´¥: {e}")
        import traceback
        print(f"è¯¦ç»†é”™è¯¯ä¿¡æ¯: {traceback.format_exc()}")
        return False

if __name__ == "__main__":
    success = asyncio.run(run_crawler())
    sys.exit(0 if success else 1)
